{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuf7D42fTwjlHNCDANLgjw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Augusto-Seixas-UFV/seixas-ufv-iac/blob/main/UFV_ELT573_BostonHousing_Relat%C3%B3rio2025_Seixas_v_Antr_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "gmDKpE4LRFZm",
        "outputId": "8fba2a46-e499-4099-affa-15a40776bd40"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1-1432429431.py, line 177)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-1432429431.py\"\u001b[0;36m, line \u001b[0;32m177\u001b[0m\n\u001b[0;31m    random_state\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Análise Completa do Boston Housing Dataset\n",
        "Disciplina: ELT 573 – Introdução ao Aprendizado Estatístico\n",
        "Implementação de Regressão Linear, Árvores, Bagging e Random Forest\n",
        "com Validação Cruzada Leave-One-Out\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import pearsonr, skew, kurtosis\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "# Configurações\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "np.random.seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. CARREGAMENTO E PREPARAÇÃO DOS DADOS\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Carrega e prepara o Boston Housing Dataset\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"1. CARREGAMENTO DO BOSTON HOUSING DATASET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Carregamento do dataset\n",
        "    boston = load_boston()\n",
        "    X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "    y = pd.Series(boston.target, name='MEDV')\n",
        "\n",
        "    print(f\"Dimensões do dataset: {X.shape}\")\n",
        "    print(f\"Variáveis explicativas: {list(X.columns)}\")\n",
        "    print(f\"Variável alvo (MEDV): min={y.min():.1f}, max={y.max():.1f}, média={y.mean():.1f}\")\n",
        "    print(f\"Valores ausentes em X: {X.isnull().sum().sum()}\")\n",
        "    print(f\"Valores ausentes em y: {y.isnull().sum()}\")\n",
        "\n",
        "    return X, y, boston\n",
        "\n",
        "# =============================================================================\n",
        "# 2. ANÁLISE EXPLORATÓRIA DOS DADOS\n",
        "# =============================================================================\n",
        "\n",
        "def exploratory_data_analysis(X, y):\n",
        "    \"\"\"Realiza análise exploratória completa dos dados\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"2. ANÁLISE EXPLORATÓRIA DOS DADOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 2.1 Estatísticas Descritivas\n",
        "    print(\"\\n2.1 ESTATÍSTICAS DESCRITIVAS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    desc_stats = X.describe()\n",
        "    print(\"Estatísticas das Variáveis Explicativas:\")\n",
        "    print(desc_stats.round(2))\n",
        "\n",
        "    print(f\"\\nEstatísticas da Variável Alvo (MEDV):\")\n",
        "    print(f\"Média: {y.mean():.2f}\")\n",
        "    print(f\"Desvio Padrão: {y.std():.2f}\")\n",
        "    print(f\"Mediana: {y.median():.2f}\")\n",
        "    print(f\"Assimetria: {skew(y):.3f}\")\n",
        "    print(f\"Curtose: {kurtosis(y):.3f}\")\n",
        "\n",
        "    # 2.2 Análise de Correlações\n",
        "    print(\"\\n2.2 ANÁLISE DE CORRELAÇÕES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Matriz de correlação completa\n",
        "    df_complete = pd.concat([X, y], axis=1)\n",
        "    correlation_matrix = df_complete.corr()\n",
        "\n",
        "    # Correlações com MEDV\n",
        "    correlations_with_target = correlation_matrix['MEDV'].drop('MEDV').sort_values(key=abs, ascending=False)\n",
        "    print(\"Correlações com MEDV (ordenadas por magnitude):\")\n",
        "    for var, corr in correlations_with_target.head(10).items():\n",
        "        print(f\"{var:8s}: {corr:6.3f}\")\n",
        "\n",
        "    # 2.3 Visualizações\n",
        "    create_exploratory_plots(X, y, correlation_matrix, correlations_with_target)\n",
        "\n",
        "    return correlation_matrix, correlations_with_target\n",
        "\n",
        "def create_exploratory_plots(X, y, correlation_matrix, correlations_with_target):\n",
        "    \"\"\"Cria visualizações para análise exploratória\"\"\"\n",
        "\n",
        "    # Figura 1: Distribuição da variável alvo\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Histograma\n",
        "    axes[0].hist(y, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0].set_title('Distribuição de MEDV')\n",
        "    axes[0].set_xlabel('Valor Médio das Casas ($1000s)')\n",
        "    axes[0].set_ylabel('Frequência')\n",
        "    axes[0].axvline(y.mean(), color='red', linestyle='--', label=f'Média: {y.mean():.1f}')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Box plot\n",
        "    axes[1].boxplot(y)\n",
        "    axes[1].set_title('Box Plot - MEDV')\n",
        "    axes[1].set_ylabel('Valor Médio das Casas ($1000s)')\n",
        "\n",
        "    # Q-Q Plot\n",
        "    stats.probplot(y, dist=\"norm\", plot=axes[2])\n",
        "    axes[2].set_title('Q-Q Plot - Normalidade de MEDV')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Figura 2: Matriz de correlação\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=0.5, mask=mask, fmt='.2f')\n",
        "    plt.title('Matriz de Correlação - Boston Housing Dataset')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Figura 3: Scatter plots das variáveis mais correlacionadas\n",
        "    top_vars = correlations_with_target.head(6).index.tolist()\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, var in enumerate(top_vars):\n",
        "        axes[i].scatter(X[var], y, alpha=0.6, color='steelblue')\n",
        "        axes[i].set_xlabel(var)\n",
        "        axes[i].set_ylabel('MEDV')\n",
        "        axes[i].set_title(f'MEDV vs {var}\\n(r = {correlation_matrix.loc[var, \"MEDV\"]:.3f})')\n",
        "\n",
        "        # Adicionar linha de tendência\n",
        "        z = np.polyfit(X[var], y, 1)\n",
        "        p = np.poly1d(z)\n",
        "        axes[i].plot(X[var], p(X[var]), \"r--\", alpha=0.8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# 3. IMPLEMENTAÇÃO DOS MODELOS\n",
        "# =============================================================================\n",
        "\n",
        "def setup_models():\n",
        "    \"\"\"Configura os modelos de machine learning\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"3. CONFIGURAÇÃO DOS MODELOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    models = {\n",
        "        'Regressão Linear': LinearRegression(),\n",
        "        'Árvore de Regressão': DecisionTreeRegressor(\n",
        "            random_state=42,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=3\n",
        "        ),\n",
        "        'Bagging': BaggingRegressor(\n",
        "            base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "            n_estimators=100,\n",
        "            random_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Análise Completa do Boston Housing Dataset\n",
        "Disciplina: ELT 573 – Introdução ao Aprendizado Estatístico\n",
        "Implementação de Regressão Linear, Árvores, Bagging e Random Forest\n",
        "com Validação Cruzada Leave-One-Out\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import pearsonr, skew, kurtosis\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "# Configurações\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "np.random.seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. CARREGAMENTO E PREPARAÇÃO DOS DADOS\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Carrega e prepara o Boston Housing Dataset\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"1. CARREGAMENTO DO BOSTON HOUSING DATASET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Carregamento do dataset\n",
        "    boston = load_boston()\n",
        "    X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "    y = pd.Series(boston.target, name='MEDV')\n",
        "\n",
        "    print(f\"Dimensões do dataset: {X.shape}\")\n",
        "    print(f\"Variáveis explicativas: {list(X.columns)}\")\n",
        "    print(f\"Variável alvo (MEDV): min={y.min():.1f}, max={y.max():.1f}, média={y.mean():.1f}\")\n",
        "    print(f\"Valores ausentes em X: {X.isnull().sum().sum()}\")\n",
        "    print(f\"Valores ausentes em y: {y.isnull().sum()}\")\n",
        "\n",
        "    return X, y, boston\n",
        "\n",
        "# =============================================================================\n",
        "# 2. ANÁLISE EXPLORATÓRIA DOS DADOS\n",
        "# =============================================================================\n",
        "\n",
        "def exploratory_data_analysis(X, y):\n",
        "    \"\"\"Realiza análise exploratória completa dos dados\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"2. ANÁLISE EXPLORATÓRIA DOS DADOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 2.1 Estatísticas Descritivas\n",
        "    print(\"\\n2.1 ESTATÍSTICAS DESCRITIVAS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    desc_stats = X.describe()\n",
        "    print(\"Estatísticas das Variáveis Explicativas:\")\n",
        "    print(desc_stats.round(2))\n",
        "\n",
        "    print(f\"\\nEstatísticas da Variável Alvo (MEDV):\")\n",
        "    print(f\"Média: {y.mean():.2f}\")\n",
        "    print(f\"Desvio Padrão: {y.std():.2f}\")\n",
        "    print(f\"Mediana: {y.median():.2f}\")\n",
        "    print(f\"Assimetria: {skew(y):.3f}\")\n",
        "    print(f\"Curtose: {kurtosis(y):.3f}\")\n",
        "\n",
        "    # 2.2 Análise de Correlações\n",
        "    print(\"\\n2.2 ANÁLISE DE CORRELAÇÕES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Matriz de correlação completa\n",
        "    df_complete = pd.concat([X, y], axis=1)\n",
        "    correlation_matrix = df_complete.corr()\n",
        "\n",
        "    # Correlações com MEDV\n",
        "    correlations_with_target = correlation_matrix['MEDV'].drop('MEDV').sort_values(key=abs, ascending=False)\n",
        "    print(\"Correlações com MEDV (ordenadas por magnitude):\")\n",
        "    for var, corr in correlations_with_target.head(10).items():\n",
        "        print(f\"{var:8s}: {corr:6.3f}\")\n",
        "\n",
        "    # 2.3 Visualizações\n",
        "    create_exploratory_plots(X, y, correlation_matrix, correlations_with_target)\n",
        "\n",
        "    return correlation_matrix, correlations_with_target\n",
        "\n",
        "def create_exploratory_plots(X, y, correlation_matrix, correlations_with_target):\n",
        "    \"\"\"Cria visualizações para análise exploratória\"\"\"\n",
        "\n",
        "    # Figura 1: Distribuição da variável alvo\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Histograma\n",
        "    axes[0].hist(y, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0].set_title('Distribuição de MEDV')\n",
        "    axes[0].set_xlabel('Valor Médio das Casas ($1000s)')\n",
        "    axes[0].set_ylabel('Frequência')\n",
        "    axes[0].axvline(y.mean(), color='red', linestyle='--', label=f'Média: {y.mean():.1f}')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Box plot\n",
        "    axes[1].boxplot(y)\n",
        "    axes[1].set_title('Box Plot - MEDV')\n",
        "    axes[1].set_ylabel('Valor Médio das Casas ($1000s)')\n",
        "\n",
        "    # Q-Q Plot\n",
        "    stats.probplot(y, dist=\"norm\", plot=axes[2])\n",
        "    axes[2].set_title('Q-Q Plot - Normalidade de MEDV')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Figura 2: Matriz de correlação\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=0.5, mask=mask, fmt='.2f')\n",
        "    plt.title('Matriz de Correlação - Boston Housing Dataset')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Figura 3: Scatter plots das variáveis mais correlacionadas\n",
        "    top_vars = correlations_with_target.head(6).index.tolist()\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, var in enumerate(top_vars):\n",
        "        axes[i].scatter(X[var], y, alpha=0.6, color='steelblue')\n",
        "        axes[i].set_xlabel(var)\n",
        "        axes[i].set_ylabel('MEDV')\n",
        "        axes[i].set_title(f'MEDV vs {var}\\n(r = {correlation_matrix.loc[var, \"MEDV\"]:.3f})')\n",
        "\n",
        "        # Adicionar linha de tendência\n",
        "        z = np.polyfit(X[var], y, 1)\n",
        "        p = np.poly1d(z)\n",
        "        axes[i].plot(X[var], p(X[var]), \"r--\", alpha=0.8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# 3. IMPLEMENTAÇÃO DOS MODELOS\n",
        "# =============================================================================\n",
        "\n",
        "def setup_models():\n",
        "    \"\"\"Configura os modelos de machine learning\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"3. CONFIGURAÇÃO DOS MODELOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    models = {\n",
        "        'Regressão Linear': LinearRegression(),\n",
        "        'Árvore de Regressão': DecisionTreeRegressor(\n",
        "            random_state=42,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=3\n",
        "        ),\n",
        "        'Bagging': BaggingRegressor(\n",
        "            base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "            n_estimators=100,\n",
        "            random_state=42,\n",
        "            max_samples=0.8\n",
        "        ),\n",
        "        'Random Forest': RandomForestRegressor(\n",
        "            n_estimators=100,\n",
        "            random_state=42,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features='sqrt'\n",
        "        )\n",
        "    }\n",
        "\n",
        "    print(\"Modelos configurados:\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"- {name}: {type(model).__name__}\")\n",
        "        if hasattr(model, 'get_params'):\n",
        "            key_params = {k: v for k, v in model.get_params().items()\n",
        "                         if k in ['max_depth', 'n_estimators', 'random_state']}\n",
        "            print(f\"  Parâmetros principais: {key_params}\")\n",
        "\n",
        "    return models\n",
        "\n",
        "# =============================================================================\n",
        "# 4. VALIDAÇÃO CRUZADA LEAVE-ONE-OUT\n",
        "# =============================================================================\n",
        "\n",
        "def perform_loocv_analysis(X, y, models):\n",
        "    \"\"\"Executa validação cruzada Leave-One-Out para todos os modelos\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"4. VALIDAÇÃO CRUZADA LEAVE-ONE-OUT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Inicialização do validador LOOCV\n",
        "    loo = LeaveOneOut()\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    print(f\"Executando LOOCV com {n_samples} iterações para cada modelo...\")\n",
        "    print(\"Isso pode levar alguns minutos...\\n\")\n",
        "\n",
        "    # Armazenamento dos resultados\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Processando {name}...\")\n",
        "\n",
        "        # Arrays para armazenar predições\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "\n",
        "        # Contador para progresso\n",
        "        count = 0\n",
        "\n",
        "        # Loop LOOCV\n",
        "        for train_idx, test_idx in loo.split(X):\n",
        "            # Divisão treino/teste\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "            # Treinamento e predição\n",
        "            model.fit(X_train, y_train)\n",
        "            pred = model.predict(X_test)\n",
        "\n",
        "            # Armazenamento\n",
        "            y_pred.extend(pred)\n",
        "            y_true.extend(y_test)\n",
        "\n",
        "            # Progresso\n",
        "            count += 1\n",
        "            if count % 100 == 0:\n",
        "                print(f\"  Processadas {count}/{n_samples} iterações\")\n",
        "\n",
        "        # Conversão para arrays numpy\n",
        "        y_pred = np.array(y_pred)\n",
        "        y_true = np.array(y_true)\n",
        "\n",
        "        # Cálculo das métricas\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        correlation, p_value = pearsonr(y_true, y_pred)\n",
        "        mae = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "        # Armazenamento dos resultados\n",
        "        results[name] = {\n",
        "            'MSE': mse,\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'Correlação': correlation,\n",
        "            'p_value': p_value,\n",
        "            'y_pred': y_pred,\n",
        "            'y_true': y_true\n",
        "        }\n",
        "\n",
        "        print(f\"  {name} concluído - MSE: {mse:.3f}, RMSE: {rmse:.3f}, r: {correlation:.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# 5. ANÁLISE DOS RESULTADOS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_results(results):\n",
        "    \"\"\"Analisa e apresenta os resultados dos modelos\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"5. ANÁLISE DOS RESULTADOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 5.1 Tabela de resultados\n",
        "    print(\"\\n5.1 COMPARAÇÃO DE PERFORMANCE\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Modelo': list(results.keys()),\n",
        "        'MSE': [results[model]['MSE'] for model in results.keys()],\n",
        "        'RMSE': [results[model]['RMSE'] for model in results.keys()],\n",
        "        'MAE': [results[model]['MAE'] for model in results.keys()],\n",
        "        'Correlação': [results[model]['Correlação'] for model in results.keys()]\n",
        "    })\n",
        "\n",
        "    # Ordenar por MSE\n",
        "    results_df = results_df.sort_values('MSE').reset_index(drop=True)\n",
        "    results_df.index = results_df.index + 1\n",
        "\n",
        "    print(results_df.round(3).to_string())\n",
        "\n",
        "    # Identificar melhor modelo\n",
        "    best_model = results_df.iloc[0]['Modelo']\n",
        "    print(f\"\\nMelhor modelo: {best_model}\")\n",
        "    print(f\"MSE: {results_df.iloc[0]['MSE']:.3f}\")\n",
        "    print(f\"Correlação: {results_df.iloc[0]['Correlação']:.3f}\")\n",
        "\n",
        "    # 5.2 Visualizações dos resultados\n",
        "    create_results_visualizations(results, results_df)\n",
        "\n",
        "    # 5.3 Análise estatística\n",
        "    perform_statistical_analysis(results)\n",
        "\n",
        "    return results_df, best_model\n",
        "\n",
        "def create_results_visualizations(results, results_df):\n",
        "    \"\"\"Cria visualizações dos resultados\"\"\"\n",
        "\n",
        "    # Figura 1: Gráficos de dispersão Predito vs Observado\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, (name, result) in enumerate(results.items()):\n",
        "        y_true = result['y_true']\n",
        "        y_pred = result['y_pred']\n",
        "\n",
        "        # Scatter plot\n",
        "        axes[i].scatter(y_true, y_pred, alpha=0.6, color='steelblue')\n",
        "\n",
        "        # Linha diagonal perfeita\n",
        "        min_val = min(min(y_true), min(y_pred))\n",
        "        max_val = max(max(y_true), max(y_pred))\n",
        "        axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Predição Perfeita')\n",
        "\n",
        "        # Configurações\n",
        "        axes[i].set_xlabel('Valores Observados')\n",
        "        axes[i].set_ylabel('Valores Preditos')\n",
        "        axes[i].set_title(f'{name}\\nMSE: {result[\"MSE\"]:.3f}, r: {result[\"Correlação\"]:.3f}')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Figura 2: Gráficos de barras comparativos\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # MSE\n",
        "    bars1 = axes[0].bar(results_df['Modelo'], results_df['MSE'], color='lightcoral', alpha=0.8)\n",
        "    axes[0].set_title('Erro Quadrático Médio (MSE)')\n",
        "    axes[0].set_ylabel('MSE')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Adicionar valores nas barras\n",
        "    for bar, value in zip(bars1, results_df['MSE']):\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                    f'{value:.1f}', ha='center', va='bottom')\n",
        "\n",
        "    # RMSE\n",
        "    bars2 = axes[1].bar(results_df['Modelo'], results_df['RMSE'], color='lightgreen', alpha=0.8)\n",
        "    axes[1].set_title('Raiz do Erro Quadrático Médio (RMSE)')\n",
        "    axes[1].set_ylabel('RMSE')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    for bar, value in zip(bars2, results_df['RMSE']):\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                    f'{value:.1f}', ha='center', va='bottom')\n",
        "\n",
        "    # Correlação\n",
        "    bars3 = axes[2].bar(results_df['Modelo'], results_df['Correlação'], color='lightblue', alpha=0.8)\n",
        "    axes[2].set_title('Correlação de Pearson')\n",
        "    axes[2].set_ylabel('Correlação')\n",
        "    axes[2].tick_params(axis='x', rotation=45)\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    axes[2].set_ylim(0, 1)\n",
        "\n",
        "    for bar, value in zip(bars3, results_df['Correlação']):\n",
        "        axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                    f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def perform_statistical_analysis(results):\n",
        "    \"\"\"Realiza análise estatística detalhada dos resultados\"\"\"\n",
        "\n",
        "    print(\"\\n5.2 ANÁLISE ESTATÍSTICA DETALHADA\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for name, result in results.items():\n",
        "        residuals = result['y_true'] - result['y_pred']\n",
        "\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Média dos resíduos: {np.mean(residuals):.4f}\")\n",
        "        print(f\"  Desvio padrão dos resíduos: {np.std(residuals):.4f}\")\n",
        "        print(f\"  Assimetria dos resíduos: {skew(residuals):.4f}\")\n",
        "        print(f\"  Curtose dos resíduos: {kurtosis(residuals):.4f}\")\n",
        "        print(f\"  R² ajustado: {result['Correlação']**2:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 6. ANÁLISE DE IMPORTÂNCIA DAS VARIÁVEIS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_feature_importance(X, y):\n",
        "    \"\"\"Analisa importância das variáveis usando Random Forest\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"6. ANÁLISE DE IMPORTÂNCIA DAS VARIÁVEIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Treinamento do Random Forest para análise de importância\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X, y)\n",
        "\n",
        "    # Importância das variáveis\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Variável': X.columns,\n",
        "        'Importância': rf_model.feature_importances_\n",
        "    }).sort_values('Importância', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"Importância das Variáveis (Random Forest):\")\n",
        "    print(feature_importance.round(4).to_string(index=False))\n",
        "\n",
        "    # Visualização\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=feature_importance, y='Variável', x='Importância', palette='viridis')\n",
        "    plt.title('Importância das Variáveis - Random Forest')\n",
        "    plt.xlabel('Importância Relativa')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Adicionar valores nas barras\n",
        "    for i, (var, imp) in enumerate(zip(feature_importance['Variável'], feature_importance['Importância'])):\n",
        "        plt.text(imp + 0.005, i, f'{imp:.3f}', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return feature_importance\n",
        "\n",
        "# =============================================================================\n",
        "# 7. ANÁLISE DE RESÍDUOS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_residuals(results, best_model):\n",
        "    \"\"\"Análise detalhada dos resíduos do melhor modelo\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"7. ANÁLISE DE RESÍDUOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    best_results = results[best_model]\n",
        "    residuals = best_results['y_true'] - best_results['y_pred']\n",
        "    y_pred = best_results['y_pred']\n",
        "\n",
        "    print(f\"Análise de resíduos para: {best_model}\")\n",
        "\n",
        "    # Figura: Análise de resíduos\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Resíduos vs Preditos\n",
        "    axes[0, 0].scatter(y_pred, residuals, alpha=0.6, color='steelblue')\n",
        "    axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
        "    axes[0, 0].set_xlabel('Valores Preditos')\n",
        "    axes[0, 0].set_ylabel('Resíduos')\n",
        "    axes[0, 0].set_title('Resíduos vs Valores Preditos')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Histograma dos resíduos\n",
        "    axes[0, 1].hist(residuals, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    axes[0, 1].set_xlabel('Resíduos')\n",
        "    axes[0, 1].set_ylabel('Frequência')\n",
        "    axes[0, 1].set_title('Distribuição dos Resíduos')\n",
        "    axes[0, 1].axvline(np.mean(residuals), color='red', linestyle='--',\n",
        "                      label=f'Média: {np.mean(residuals):.3f}')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Q-Q Plot dos resíduos\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=axes[0, 2])\n",
        "    axes[0, 2].set_title('Q-Q Plot - Normalidade dos Resíduos')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Resíduos padronizados\n",
        "    std_residuals = residuals / np.std(residuals)\n",
        "    axes[1, 0].scatter(y_pred, std_residuals, alpha=0.6, color='orange')\n",
        "    axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
        "    axes[1, 0].axhline(y=2, color='red', linestyle=':', alpha=0.6, label='±2σ')\n",
        "    axes[1, 0].axhline(y=-2, color='red', linestyle=':', alpha=0.6)\n",
        "    axes[1, 0].set_xlabel('Valores Preditos')\n",
        "    axes[1, 0].set_ylabel('Resíduos Padronizados')\n",
        "    axes[1, 0].set_title('Resíduos Padronizados')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Resíduos absolutos vs Preditos\n",
        "    abs_residuals = np.abs(residuals)\n",
        "    axes[1, 1].scatter(y_pred, abs_residuals, alpha=0.6, color='purple')\n",
        "    axes[1, 1].set_xlabel('Valores Preditos')\n",
        "    axes[1, 1].set_ylabel('|Resíduos|')\n",
        "    axes[1, 1].set_title('Resíduos Absolutos vs Preditos')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Box plot dos resíduos\n",
        "    axes[1, 2].boxplot(residuals)\n",
        "    axes[1, 2].set_title('Box Plot dos Resíduos')\n",
        "    axes[1, 2].set_ylabel('Resíduos')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Teste de normalidade dos resíduos\n",
        "    from scipy.stats import shapiro, jarque_bera\n",
        "\n",
        "    print(f\"\\nTestes de Normalidade dos Resíduos:\")\n",
        "\n",
        "    # Teste de Shapiro-Wilk\n",
        "    shapiro_stat, shapiro_p = shapiro(residuals)\n",
        "    print(f\"Shapiro-Wilk: estatística = {shapiro_stat:.4f}, p-valor = {shapiro_p:.4f}\")\n",
        "\n",
        "    # Teste de Jarque-Bera\n",
        "    jb_stat, jb_p = jarque_bera(residuals)\n",
        "    print(f\"Jarque-Bera: estatística = {jb_stat:.4f}, p-valor = {jb_p:.4f}\")\n",
        "\n",
        "    # Identificar outliers\n",
        "    outliers = np.where(np.abs(std_residuals) > 2)[0]\n",
        "    print(f\"\\nOutliers identificados (|resíduo padronizado| > 2): {len(outliers)} observações\")\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"Índices dos outliers: {outliers[:10]}{'...' if len(outliers) > 10 else ''}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 8. FUNÇÃO PRINCIPAL\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Função principal que executa toda a análise\"\"\"\n",
        "\n",
        "    print(\"ANÁLISE COMPLETA DO BOSTON HOUSING DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Disciplina: ELT 573 – Introdução ao Aprendizado Estatístico\")\n",
        "    print(\"Comparação de Modelos com Validação Cruzada Leave-One-Out\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        # 1. Carregamento dos dados\n",
        "        X, y, boston_data = load_and_prepare_data()\n",
        "\n",
        "        # 2. Análise exploratória\n",
        "        correlation_matrix, correlations_target = exploratory_data_analysis(X, y)\n",
        "\n",
        "        # 3. Configuração dos modelos\n",
        "        models = setup_models()\n",
        "\n",
        "        # 4. Validação cruzada LOOCV\n",
        "        results = perform_loocv_analysis(X, y, models)\n",
        "\n",
        "        # 5. Análise dos resultados\n",
        "        results_df, best_model = analyze_results(results)\n",
        "\n",
        "        # 6. Importância das variáveis\n",
        "        feature_importance = analyze_feature_importance(X, y)\n",
        "\n",
        "        # 7. Análise de resíduos\n",
        "        analyze_residuals(results, best_model)\n",
        "\n",
        "        # 8. Relatório final\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RELATÓRIO FINAL\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Dataset: {X.shape[0]} observações, {X.shape[1]} variáveis\")\n",
        "        print(f\"Melhor modelo: {best_model}\")\n",
        "        print(f\"MSE do melhor modelo: {results[best_model]['MSE']:.3f}\")\n",
        "        print(f\"Correlação do melhor modelo: {results[best_model]['Correlação']:.3f}\")\n",
        "        print(f\"Variável mais importante: {feature_importance.iloc[0]['Variável']}\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"Análise concluída com sucesso!\")\n",
        "\n",
        "        return results, results_df, feature_importance\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro durante a execução: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None\n",
        "\n",
        "# =============================================================================\n",
        "# EXECUÇÃO\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, results_df, feature_importance = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ouN0siq6RQlW",
        "outputId": "2dc40a49-221b-43d8-ac3e-62c45f9a3468"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-566061535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}